{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-06T06:44:33.180446Z",
     "start_time": "2025-02-06T06:44:30.754027Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "#计算softmax举例\n",
    "x = torch.tensor([[100, 200]], dtype=torch.float32)\n",
    "x_softmax = torch.softmax(x, dim=1)\n",
    "print(x_softmax)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.7835e-44, 1.0000e+00]])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T06:44:58.742627Z",
     "start_time": "2025-02-06T06:44:58.732838Z"
    }
   },
   "cell_type": "code",
   "source": "x.shape",
   "id": "3e99443a9959360c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T06:45:06.365821Z",
     "start_time": "2025-02-06T06:45:06.359964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[1,2 ]], dtype=torch.float32)\n",
    "x_softmax = torch.softmax(x, dim=1)\n",
    "print(x_softmax)"
   ],
   "id": "14838e8eb306fe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2689, 0.7311]])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T07:15:21.968557Z",
     "start_time": "2025-02-06T07:15:21.960401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def custom_embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False,\n",
    "                     sparse=False):\n",
    "    # max_norm限制了最大范数，norm_type指定范数计算方式，scale_grad_by_freq是否按词频缩放梯度\n",
    "    \"\"\"\n",
    "    A simplified version of torch.nn.functional.embedding.\n",
    "    \"\"\"\n",
    "    # 输入检查\n",
    "    if not isinstance(input, torch.LongTensor):\n",
    "        raise TypeError(\"Input must be a LongTensor\")\n",
    "\n",
    "    if padding_idx is not None:\n",
    "        if padding_idx >= weight.size(0) or padding_idx < 0:\n",
    "            raise ValueError(\"padding_idx must be within the range of weight size\")\n",
    "\n",
    "    # 获取嵌入矩阵的形状\n",
    "    num_embeddings, embedding_dim = weight.size()\n",
    "\n",
    "    # 初始化输出张量\n",
    "    output = torch.zeros(input.size() + (embedding_dim,), dtype=weight.dtype, device=weight.device)   \n",
    "\n",
    "    # 遍历输入张量的每个索引\n",
    "    for idx in input.view(-1).tolist(): # 展开输入张量，并转换为列表\n",
    "        if idx != padding_idx:\n",
    "            output.view(-1, embedding_dim)[idx] = weight[idx]   # 填充输出张量\n",
    "\n",
    "    # 如果指定了 max_norm，对嵌入向量进行归一化\n",
    "    if max_norm is not None:\n",
    "        with torch.no_grad():\n",
    "            norm = weight.norm(p=norm_type, dim=1, keepdim=True)     # 计算weight的范数\n",
    "            torch.clamp(norm, max=max_norm, out=norm)     # 限制范数的最大值\n",
    "            weight.div_(norm)   # 对weight进行归一化\n",
    "\n",
    "    return output   # output的形状为 (batch_size * seq_len * embedding_dim)\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "input_indices = torch.tensor([[1, 2, 4], [4, 3, 2]], dtype=torch.long)  # shape: (batch_size, seq_len)\n",
    "embedding_matrix = torch.randn(5, 10)  # 假设有 5 个词，每个词的嵌入维度为 10  shape: (vocab_size, embedding_dim)\n",
    "\n",
    "output = custom_embedding(input_indices, embedding_matrix)\n",
    "print(output)   # shape: (batch_size, seq_len, embedding_dim)\n",
    "output.shape"
   ],
   "id": "7c25690d32918296",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000],\n",
      "         [-0.1098, -0.2201, -1.2564, -0.9372, -1.8769,  1.7540, -0.6517,\n",
      "           0.3467,  0.8380,  0.4739],\n",
      "         [ 0.9540,  0.2696, -1.5430, -0.4435,  0.5579, -0.1922,  0.2865,\n",
      "           1.8943,  0.0369,  0.8779]],\n",
      "\n",
      "        [[-0.8891,  1.1431,  1.5063, -0.3315,  0.2334,  1.1230,  0.4451,\n",
      "          -0.6990,  0.0068,  0.8159],\n",
      "         [ 0.1075,  0.8889, -1.5297,  1.5845, -0.3677,  1.6821, -1.0814,\n",
      "           0.3575, -0.3537, -0.2370],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
