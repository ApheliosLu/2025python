{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:48:38.784766Z",
     "start_time": "2025-01-18T00:48:38.775271Z"
    }
   },
   "source": [
    "import torch"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch 求导\n",
    "\n",
    "参考 [url](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
    "\n",
    "pytorch 实现模型训练需要完整地写下训练过程，包括反向传播求梯度以及应用梯度下降算法。（06见chapter_2/03_...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 近似求导"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:48:38.802907Z",
     "start_time": "2025-01-18T00:48:38.790772Z"
    }
   },
   "source": [
    "def f(x):\n",
    "    return 3. * x ** 2 + 2. * x - 1\n",
    "#近视求导，x移动eps单位，也就是离自己很近的一个点的切线\n",
    "def approximate_derivative(f, x, eps=1e-6):\n",
    "    return (f(x + eps) - f(x - eps)) / (2. * eps)\n",
    "\n",
    "print(approximate_derivative(f, 1.))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.999999999785956\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:48:38.824882Z",
     "start_time": "2025-01-18T00:48:38.822051Z"
    }
   },
   "source": [
    "#求偏导数,其中一个数不动，对另外一个变量求导\n",
    "def g(x1, x2):\n",
    "    return (x1 + 5) * (x2 ** 2)\n",
    "\n",
    "def approximate_gradient(g, x1, x2, eps=1e-3):\n",
    "    dg_x1 = approximate_derivative(lambda x: g(x, x2), x1, eps)\n",
    "    dg_x2 = approximate_derivative(lambda x: g(x1, x), x2, eps)\n",
    "    return dg_x1, dg_x2\n",
    "\n",
    "print(approximate_gradient(g, 2., 3.))\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8.999999999993236, 41.999999999994486)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch 近似求导"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:48:38.841341Z",
     "start_time": "2025-01-18T00:48:38.837906Z"
    }
   },
   "source": [
    "# 声明两个tensor x1 和 x2，允许梯度计算，使用torch的自动求导上下文计算两个tensor的梯度\n",
    "# 使用 torch.autograd.grad 计算 y = g(x1, x2) 的偏导数\n",
    "\n",
    "x1 = torch.tensor([2.], requires_grad=True)\n",
    "x2 = torch.tensor([3.], requires_grad=True)\n",
    "y = g(x1, x2)\n",
    "    \n",
    "(dy_dx1,) = torch.autograd.grad(y, x1,retain_graph=True)\n",
    "print(dy_dx1)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:48:38.855092Z",
     "start_time": "2025-01-18T00:48:38.841341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try: #不加retain_graph=True第二次执行会报错，原因是因为计算图已经被释放了\n",
    "    (dy_dx2,) = torch.autograd.grad(y, x2,retain_graph=True)\n",
    "    print(dy_dx2)\n",
    "except Exception as e:\n",
    "    print(e)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([42.])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:48:38.863693Z",
     "start_time": "2025-01-18T00:48:38.855092Z"
    }
   },
   "source": [
    "# 同时求导\n",
    "\n",
    "x1 = torch.tensor([2.], requires_grad=True)\n",
    "x2 = torch.tensor([3.], requires_grad=True)\n",
    "y = g(x1, x2)\n",
    "\n",
    "# 求偏导数\n",
    "dy_dx1, dy_dx2 = torch.autograd.grad(y, [x1, x2])\n",
    "\n",
    "\n",
    "print(dy_dx1, dy_dx2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.]) tensor([42.])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:48:38.870707Z",
     "start_time": "2025-01-18T00:48:38.863693Z"
    }
   },
   "source": [
    "# 当然我们一般直接用 backward\n",
    "\n",
    "x1 = torch.tensor([2.], requires_grad=True)\n",
    "x2 = torch.tensor([3.], requires_grad=True)\n",
    "y = g(x1, x2)\n",
    "\n",
    "# 求偏导数,求梯度\n",
    "y.backward()\n",
    "print(x1.grad, x2.grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.]) tensor([42.])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二阶导\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:48:38.878163Z",
     "start_time": "2025-01-18T00:48:38.870707Z"
    }
   },
   "source": [
    "x1 = torch.tensor([2.], requires_grad=True)\n",
    "x2 = torch.tensor([3.], requires_grad=True)\n",
    "y = g(x1, x2)\n",
    "\n",
    "# 求y对x1和x2的二阶偏导数\n",
    "#，allow_unused 参数的作用是控制当 inputs 中的某些张量不需要梯度时，函数的行为方式。\n",
    "# allow_unused=True 这个参数的作用是 允许计算图中某些张量没有被使用，即在计算梯度时允许部分张量的梯度不被计算或为 None。\n",
    "dy_dx1, dy_dx2 = torch.autograd.grad(y, [x1, x2], create_graph=True)\n",
    "dy_dx1_dx1, dy_dx1_dx2 = torch.autograd.grad(dy_dx1, [x1, x2], allow_unused=True)\n",
    "dy_dx2_dx1, dy_dx2_dx2 = torch.autograd.grad(dy_dx2, [x1, x2], allow_unused=True)\n",
    "print(dy_dx1_dx1, dy_dx2_dx1, dy_dx2_dx2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None tensor([6.]) tensor([14.])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:48:38.894360Z",
     "start_time": "2025-01-18T00:48:38.885668Z"
    }
   },
   "source": [
    "#模拟梯度下降算法 SGD\n",
    "import torch\n",
    "learning_rate = 0.3 # 每次参数更新时的步长\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "for _ in range(100):# 下划线 _ 是一个惯用法，表示 占位符，即它用于指代一个不需要用到的变量。range(100) 会返回 0 到 99 的数字。如果我们不关心这些数字的具体值（即不需要用到它们），就可以使用 _ 来代替变量名。\n",
    "    z = f(x)\n",
    "    z.backward()    # 计算 z 对所有涉及的张量的梯度。在本例中，z 是通过 x 计算得到的，所以 x 的梯度会被计算出来，并存储在 x.grad 中。\n",
    "    x.data.sub_(learning_rate * x.grad) # x -= learning_rate * x.grad，这里就等价于optimizer.step()\n",
    "    x.grad.zero_() # x.grad -= x.grad, x.grad = 0,梯度清零\n",
    "    # 由于 PyTorch 会自动累积梯度（即每次调用 backward() 时，梯度会被加到现有的梯度上），所以在每次更新参数后，我们需要 手动清零梯度，以便下次计算时不会受到前次梯度的影响。\n",
    "print(x)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3333, requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "a=torch.tensor(2) # 标量\n",
    "a.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-18T00:48:38.921899Z",
     "start_time": "2025-01-18T00:48:38.917503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T00:48:38.997780Z",
     "start_time": "2025-01-18T00:48:38.933215Z"
    }
   },
   "source": [
    "#GradientTape与optimizer（优化器）结合使用\n",
    "learning_rate = 0.01\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([x], lr=learning_rate,momentum=0.9)\n",
    "for _ in range(500):\n",
    "    z = f(x)\n",
    "    optimizer.zero_grad() # 梯度变为0\n",
    "    z.backward() # dz/dx,求梯度\n",
    "    # print(x.grad)\n",
    "    optimizer.step() # x -= learning_rate * x.grad\n",
    "    \n",
    "\n",
    "print(x)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3333, requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
